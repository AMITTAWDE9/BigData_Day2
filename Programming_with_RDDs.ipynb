{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Apache Spark?\n",
    "\n",
    "**Apache Spark is a cluster computing platform designed to be fast and general-purpose.**\n",
    "\n",
    "- On the speed side, Spark extends the popular MapReduce model to efficiently support more types of computations, including interactive queries and stream processing.\n",
    "\n",
    "- One of the main features Spark offers for speed is the ability to run computations in memory, but the system is also more efficient than MapReduce for complex applications running on disk.\n",
    "\n",
    "- At its core, Spark is a **“computational engine”** that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/apache-spark-vs-hadoop-mapreduce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark: Data Science tasks\n",
    "\n",
    "- Data scientists job includes experience with SQL, statistics, predictive modeling (machine learning), and programming, usually in Python, Matlab, or R. Data scientists also have experience with techniques necessary to transform data into formats that can be analyzed for insights (sometimes referred to as data wrangling)\n",
    "\n",
    "\n",
    "- Spark’s speed and simple APIs can make their life bit easy, and its built-in libraries mean that many algorithms are available out of the box.\n",
    "\n",
    "- The Spark shell makes it easy to do interactive data analysis using Python or Scala.\n",
    "\n",
    "- Spark SQL also has a separate SQL shell that can be used to do data exploration using SQL, or Spark SQL can be used as part of a regular Spark program or in the Spark shell.\n",
    "\n",
    "- Machine learning and data analysis is supported through the MLLib libraries or you can load your own models too.\n",
    "\n",
    "- Spark provides a simple way to parallelize these applications across clusters, and hides the complexity of distributed systems programming, network communication, and fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage layers for Spark\n",
    "\n",
    "- Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS) or other storage systems supported by the Hadoop APIs (including local filesystem, Amazon S3, Cassandra, Hive, HBase, etc.).\n",
    "\n",
    "*Note: Spark does not require Hadoop; it simply has support for storage systems implementing the Hadoop APIs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Visit Apache Spark Downloads page\n",
    "\n",
    "    http://spark.apache.org/downloads.html\n",
    "    \n",
    "\n",
    "2. Select following options\n",
    "    1. Choose a Spark release: **2.2.x** or greater (I'll be using 2.2.1)\n",
    "    2. Choose a package type:  **Pre-built for Apache Hadoop 2.7 and later**\n",
    "    3. Download Spark: **spark-2.2.1-bin-hadoop2.7.tgz**\n",
    "\n",
    "    Download that tar compressed file to your local machine.\n",
    "\n",
    "3. After downloading the compressed file, unzip it to desired location:\n",
    "\n",
    "    `$ tar -xvzf spark-2.2.1-bin-hadoop2.7.tgz -C /home/prakshi/spark/`\n",
    "\n",
    "4. Setting up the environment for Spark:\n",
    "\n",
    "    To set up environment variable:\n",
    "    \n",
    "    Add following lines to your `~/.bashrc`\n",
    "\n",
    "    ```bash\n",
    "    export SPARK_HOME=/home/prakshi/tools/spark\n",
    "    export PATH=$SPARK_HOME/bin:$PATH\n",
    "    ```\n",
    "    Make sure you change the path in `SPARK_HOME` as per your spark software file are located.\n",
    "    Reload your `~/.bashrc` file using:\n",
    "\n",
    "    ```\n",
    "    $ source ~/.bashrc\n",
    "    ```\n",
    "\n",
    "5. That's all! Spark has been set-up. Try running `pyspark` command to use Spark from Python.\n",
    "\n",
    "\n",
    "## Pyspark in Jupyter Notebook\n",
    "\n",
    "Two methods to do so.\n",
    "\n",
    "1. Configure PySpark driver\n",
    "    Update PySpark driver environment variables: add these lines to your ~/.bashrc (or ~/.zshrc) file.\n",
    "\n",
    "    ```bash\n",
    "    export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "    export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "    ```\n",
    "    \n",
    "    Restart your terminal and launch PySpark again:\n",
    "\n",
    "    `$ pyspark`\n",
    "\n",
    "    Now, this command should start a Jupyter Notebook in your web browser.\n",
    "\n",
    "2. Using `findspark` module\n",
    "    \n",
    "    findSpark package is not specific to Jupyter Notebook, you can use this trick in your favorite IDE too.\n",
    "\n",
    "    To install findspark:\n",
    "\n",
    "    `$ pip install findspark`\n",
    "\n",
    "    Irrespective of Jupyter notebook/Python script all you need to do to use spark is add following line in your code:\n",
    "\n",
    "    ```python\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD: Resilient distributed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "1.1 What are RDDs?\n",
    "\n",
    "1.2 Introduction to the problem\n",
    "\n",
    "1.3 Creating RDDs\n",
    "\n",
    "1.4 Operations on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 What are RDDs?\n",
    "\n",
    "-  RDDs stands for Resilient Distributed Dataset.\n",
    "\n",
    "- RDD is the **core abstraction** in Apache Spark.\n",
    "(In case you are thinking, What does abstraction mean in programming? then go through this excellent expalnation over stackoverflow - [Abstraction](https://stackoverflow.com/questions/21220155/what-does-abstraction-mean-in-programming)\n",
    "\n",
    "- An RDD is simply a **immutable distributed collection of elements**. \n",
    "\n",
    "- The name captures two important properties:\n",
    "    - **Resilient** means that we must be able to withstand failures and complete an ongoing computation.\n",
    "    - **Distributed** means that we must account for multiple machines having a subset of data. Formally, RDD is a read-only, partitioned collection of records\n",
    "\n",
    "- In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result.\n",
    "\n",
    "- The data inside a spark application is read into the form of RDDs and then Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n",
    "\n",
    "- RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "## 1.2 Introduction to the problem\n",
    "\n",
    "### PageRank\n",
    "\n",
    "We live in a computer era. Internet is part of our everyday lives and information is only a click away. Just open your favorite search engine, like Google, AltaVista, Yahoo, type in the key words, and the search engine will display the pages relevant for your search. But how does a search engine really work?\n",
    "\n",
    "The usefulness of a search engine depends on the relevance of the result set it gives back. There may of course be millions of web pages that include a particular word or phrase; however some of them will be more relevant, popular, or authoritative than others. A user does not have the ability or patience to scan through all pages that contain the given query words. One expects the relevant pages to be displayed within the top 20-30 pages returned by the search engine. Modern search engines employ methods of ranking the results to provide the \"best\" results first that are more elaborate than just plain text ranking.\n",
    "\n",
    "So, our aim is to **Rank the webpages to provide best search results.**\n",
    "\n",
    "For this, we will use Page Rank Algorithm. The idea that Page Rank introduce is that, the importance of any web page can be judged by looking at the pages that link to it. Let's take an example to understand the algorithm better:\n",
    "\n",
    "<img src = \"images/pagerank.jpg\">\n",
    "\n",
    "\n",
    "### How the Algorithm Works?\n",
    "\n",
    "The PageRank algorithm outputs a probability distribution that represents the likelihood that a person randomly clicking on web links will arrive at a particular web page. If we run the PageRank program with the input data file and indicate 20 iterations we shall get the following output: \n",
    "\n",
    " ```\n",
    " url_4 has rank: 1.3705281840649928.\n",
    " url_2 has rank: 0.4613200524321036.\n",
    " url_3 has rank: 0.7323900229505396.\n",
    " url_1 has rank: 1.4357617405523626.\n",
    " ```\n",
    "The results clearly indicates that URL_1 has the highest page rank followed by URL_4 and then URL_3 & last URL_2. The algorithm works in the following manner:\n",
    "\n",
    "If a URL (page) is referenced the most by other URLs then its rank increases, because being referenced means that it is important which is the case of URL_1.\n",
    "If an important URL like URL_1 references other URLs like URL_4 this will increase the destination’s ranking.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction to the Dataset\n",
    "\n",
    "Throughout the entire concept, we will be using MEDLINE records (abstracts in the life sciences domain). \n",
    "The links represent content-similarity links, i.e., pairs of abstracts that are similar in the words they contain. For example, consider pmid (unique identifier in the MEDLINE collection) [8709207](https://www.ncbi.nlm.nih.gov/pubmed/8709207). See the \"Related Links\" panel on the right hand side of the browser? The data provided above represent instances of graphs defined by such links.\n",
    "\n",
    "The files are tab-delimited adjacency list representations of the link between these medical records and hence their webpages. The first token on each line represents the unique id of the source node, and the rest of the tokens represent the target nodes (i.e., outlinks from the source node). If a node does not have any outlinks, its corresponding line will contain only one token (the source node id).\n",
    "\n",
    "\n",
    "### Let's get started\n",
    "\n",
    "**Treat**: The function implementing the algorithm logic will be provided :) \n",
    "\n",
    "**Task**: Read the dataset into RDDs and implement a simpler version of PageRank in PySpark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating RDDs\n",
    "\n",
    "Spark provides two ways to create RDDs:\n",
    "\n",
    "- Parallelizing a collection in your driver program.\n",
    "- Loading an external dataset\n",
    "\n",
    "### Parallelizing a collection in your driver program:\n",
    "\n",
    "Create RDDs using parallelize() method on existing iterable or collection in your driver program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context\n",
    "\n",
    "If you have configured Apache Spark on your machine successfully then go ahead and type command `pyspark` to open up Ipython notebook in your browser.\n",
    "\n",
    "Since we initiated from `pyspark` command, instance of `SparkContext` will be available to us. \n",
    "If you have followed another method then use `findspark`. Initiating `SparkContext` in that case would be as follows:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "```\n",
    "\n",
    "> you may need to pass `spark_home` path to `init` method call\n",
    "\n",
    "Example:\n",
    "\n",
    "`findspark.init(spark_home='path_to_spark')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test to see if you have access to `sc` variable which is `SparkContext` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.104:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/sc.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role of a Spark Context\n",
    "\n",
    "**Spark Context is the entry point. Like a key to your car.**\n",
    "\n",
    "- Tells Sparks how to access a cluster\n",
    "\n",
    "- Allocates Executors\n",
    "\n",
    "-  The context, living in your driver program, coordinates sets of processes on the cluster to run your application.\n",
    "\n",
    "- The context keeps track of live executors by sending heartbeat messages periodically. \n",
    "\n",
    "\n",
    "- Third, the context may perform dynamic resource allocation if the cluster manager permits. This increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands.\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "In the sense, if you want to compute a complex aggregation on spark, you need to distribute the task in the cluster.\n",
    "\n",
    "Spark context is the gateway to a Spark Cluster.\n",
    "\n",
    "**Example 2:**\n",
    "\n",
    "If you have a dataset ( simple CSV/TXT file) and want computations on this data, you want all the worker nodes to have access to this data. Use the spark context to broadcast this file to all the nodes.\n",
    "\n",
    "- It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The resource manager can be one of these three- Spark Standalone, YARN, Apache Mesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "Number of partitions in your RDD is  4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "\n",
    "# splitting into 4 slices\n",
    "wordsRDD = sc.parallelize(wordsList, 4)\n",
    "\n",
    "# Print out the type of wordsRDD\n",
    "print(type(wordsRDD))\n",
    "\n",
    "# Print number of partitions in your RDD\n",
    "num_partitions = wordsRDD.getNumPartitions()\n",
    "print(\"Number of partitions in your RDD is  %s\" % (num_partitions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an external dataset\n",
    "\n",
    "**Two methods:** These methods takes an URI for the file (either a local path on the machine, or a hdfs://, s3n://, etc URI).\n",
    "    - `sc.textFile()`: Creates a RDD with each line as an element. \n",
    "    - `sc.wholeTextFiles()` :  Creates a PairRDD with the key being the file name with a path.\n",
    "I'll be using a similar dataset to explain the concepts and same operations can be applied on the medline dataset to achieve solution to our problem.\n",
    "\n",
    "**Example dataset** - `data/example_pager.txt`\n",
    "\n",
    "**Medline dataset** - `data/medline_pager.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'url_1\\turl_4',\n",
       " u'url_2\\turl_1',\n",
       " u'url_3\\turl_2\\turl_1',\n",
       " u'url_4\\turl_3\\turl_1']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using sc.textfile()\n",
    "\n",
    "url_rdd = sc.textFile(\"data/sample_pager.txt\")\n",
    "url_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, create a RDD from Medline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_rdd = sc.textFile(\"data/medline_pager.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of partitions in your RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_num_partitions = med_rdd.getNumPartitions()\n",
    "\n",
    "print (\"Number of partitions in your Medline RDD is  %s\" % (med_num_partitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Operations on RDDs\n",
    "\n",
    "From the Spark Programming Guide:\n",
    "\n",
    ">RDDs support two types of operations: **transformations**, which create a new dataset from an existing one, and **actions**, which return a value to the driver program after running a computation on the dataset.\n",
    "\n",
    "### Transformation on RDDs\n",
    "\n",
    "- Transformations are kind of operations which will transform your RDD data from one form to another. And when you apply this operation on any RDD, you will get a new RDD with transformed data\n",
    "\n",
    "- Operations like map, filter, flatMap are transformations.\n",
    "\n",
    "- **Note:** when you apply the transformation on any RDD it will not perform the operation immediately. It will create a DAG(Directed Acyclic Graph) using the applied operation, source RDD and function used for transformation. And it will keep on building this graph using the references till you apply any action operation on the last lined up RDD. That is why the transformation in Spark are lazy.\n",
    "\n",
    "\n",
    "### Actions on RDDs\n",
    "- This kind of operation will also give you another RDD but this operation will trigger all the lined up transformation on the base RDD (or in the DAG) and than execute the action operation on the last RDD. \n",
    "- Operations like collect, take, count, first, saveAsTextFile are actions\n",
    "\n",
    "\n",
    "Let's have a look to data in your RDD currently looks like. We will use `take()` method for this:\n",
    "\n",
    "**`take(n)`**:\n",
    "\n",
    "The action take(n) returns n number of elements from RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'url_1', u'url_4'],\n",
       " [u'url_2', u'url_1'],\n",
       " [u'url_3', u'url_2', u'url_1'],\n",
       " [u'url_4', u'url_3', u'url_1']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how your medline RDD looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'9627181\\t10027417\\t8618855\\t9562469\\t12135350\\t8980023',\n",
       " u'9470136\\t9427340\\t8747858\\t7891871\\t10208640\\t7992850']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this RDD contains on each line, first element as page name and rest list of all neighbors and everything tab seperated.\n",
    "\n",
    "**Generate a web system as an RDD as page name, neighbor page name from the current RDD (page name, list of all neighbors).**\n",
    "\n",
    "- For this, we have to call transformations on our current RDD. First let's create a simple list out of each line removing tabs.\n",
    "\n",
    "**`map():`**\n",
    "The map() transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD.\n",
    "\n",
    "\n",
    "**`filter():`**\n",
    "The filter() transformation takes in a function and returns an RDD that only has elements that pass the filter() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'url_1', u'url_4'],\n",
       " [u'url_2', u'url_1'],\n",
       " [u'url_3', u'url_2', u'url_1'],\n",
       " [u'url_4', u'url_3', u'url_1']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_rdd = url_rdd.map(lambda line: line.split('\\t'))\n",
    "url_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's call a customize function to get the RDD in the required format. Note here we want to produce multiple output elements for each input element. The operation to do this is called **flatMap()**.\n",
    "\n",
    "**flatMap():** Similar to map, it returns a new RDD by applying a function to each element of the RDD, but output is flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'url_1', u'url_4'],\n",
       " [u'url_2', u'url_1'],\n",
       " [u'url_3', u'url_2'],\n",
       " [u'url_3', u'url_1'],\n",
       " [u'url_4', u'url_3'],\n",
       " [u'url_4', u'url_1']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ungroup_weblinks(each_pager):\n",
    "    page_main = each_pager[0]\n",
    "    final_list = []\n",
    "    for i in range(len(each_pager)-1):\n",
    "        final_list.append([page_main, each_pager[i+1]])\n",
    "    return final_list\n",
    "\n",
    "\n",
    "url_links_rdd = url_rdd.flatMap(lambda line: ungroup_weblinks(line))\n",
    "\n",
    "url_links_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, you'll see the ouput is flattened as expected.\n",
    "\n",
    "Do the same operation on `med_rdd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our workflow to solve this problem will be like as shown below:**\n",
    "\n",
    "\n",
    "<img src = \"images/pgalgo.jpg\">\n",
    "\n",
    "**Let's start with creating the links RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Create url link Paired RDD\n",
    "\n",
    "#### PairedRDDs: Working with Key/Value pairs\n",
    "\n",
    "\n",
    "**Paired RDD**: Pair RDD is just a way of referring to an RDD containing key/value pairs, i.e. tuples of data.\n",
    "\n",
    "- Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key. \n",
    "\n",
    "- We will create a paired RDD from our existing RDD in the next steps\n",
    "\n",
    "**Now, Generate the websystem as an RDD of tuples (page, list of neighbors) from the `url_links_rdd` i.e create a PairedRDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides us with a direct method to group the elements in a RDD by using `groupByKey()/reduceByKey()` method.\n",
    "\n",
    "**groupByKey()**:  We can apply the “groupByKey” / “reduceByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    return urls[0], urls[1]\n",
    "\n",
    "\n",
    "url_links_rdd = url_links_rdd.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "# pg.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check if we got the expected RDD!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'url_3', [u'url_1', u'url_2']),\n",
       " (u'url_1', [u'url_4']),\n",
       " (u'url_4', [u'url_3', u'url_1']),\n",
       " (u'url_2', [u'url_1'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url_links_rdd now contains tuples (page, list of neighbors)\n",
    "url_links_rdd.mapValues(list).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `cache()` method above. **Can you tell why?**\n",
    "\n",
    "Hint - Links RDD will be called multiple times in our solution\n",
    "\n",
    "Answer - When you run a spark transformation via an action (count, print, foreach), then, and only then is your graph being materialized and in your case the file is being consumed. RDD.cache purpose it to make sure that the result of sc.textFile(\"data.txt\") is available in memory and isn't needed to be read over again. In our case we want transformed url link rdd to cached.\n",
    "\n",
    "When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[25] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking keys of a PairedRDD\n",
    "url_links_rdd.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations are again grouped into two types:**\n",
    "\n",
    "- **Narrow transformation:** RDD operations like map, union, filter can operate on a single partition and map the data of that partition to resulting single partition. These kind of operations which maps data from one to one partition are referred as Narrow operations. Narrow operations doesn’t required to distribute the data across the partitions.\n",
    "\n",
    "\n",
    "- **Wide transformation** -RDD operations like groupByKey, distinct, join may require to map the data across the partitions in new RDD. These kind of operations which maps data from one to many partitions are referred as Wide operations. Narrow operations doesn’t required to distribute the data across the partitions. In most of the cases Wide operations distribute the data across the partitions. These considered to be more costly than narrow operations due to data shuffling.\n",
    "\n",
    "<img src= \"images/transformations.jpg\">\n",
    "\n",
    "**Note: Wide transformations results in stages**\n",
    "\n",
    "\n",
    "Data materialization occurs in a few places. When reading, shuffling, or passing data to an action. This is where the distinction between **narrow and wide dependencies** comes up. \n",
    "Narrow dependencies allow pipelining, a state of flow is straightforward. While wide dependencies forbid pipelining by requiring a shuffle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Populating the Ranks Data - Initial Seeds\n",
    "\n",
    "The code below creates \"ranks0\" - a key/value pair RDD by taking the key (URL) from the links RDD and assigning the value = 1.0 to it. Ranks0 is the initial ranks RDD and it is populated with the seed number 1.0 (please see diagram below). In the 3rd part of the program we shall see how this ranks RDD is recalculated at each iteration and eventually converges, after 20 iterations, into the PageRank probability scores mentioned previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = url_links_rdd.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'url_3', 1.0), (u'url_1', 1.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Looping and Calculating Contributions & Recalculating Ranks\n",
    "\n",
    "\n",
    "This part is the heart of the PageRank algorithm. In each iteration, the contributions are calculated and the ranks are recalculated based on those contributions. The algorithm has 4 steps: \n",
    "\n",
    "1- Start the algorithm with each page at rank 1 \n",
    "\n",
    "2- Calculate URL contribution: contrib = rank/size \n",
    "\n",
    "3- Set each URL new rank = 0.15 + 0.85 x contrib \n",
    "\n",
    "4- Iterate to step 2 with the new rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the formula given above to calculate the PageRanks\n",
    "\n",
    "**Transformations on a Paired RDD**\n",
    "\n",
    "**`mapValues():`** \n",
    "\n",
    "- When we use map() with a Pair RDD, we get access to both Key & value. There are times we might only be interested in accessing the value(& not key). In those case, we can use mapValues() instead of map().\n",
    "\n",
    "- Apply a function to each value of a pair RDD without changing the key.\n",
    "\n",
    "**`reduceByKey()`**:\n",
    "- Combine values with the same key.\n",
    "\n",
    "- It is a wide operation as it shuffles data from multiple partitions and creates another RDD\n",
    "\n",
    "- Before sending data across the partitions, it also merges the data locally using the same associative function for optimized data shuffling\n",
    "\n",
    "**`join()`**:\n",
    "\n",
    "- Some of the most useful operations we get with keyed data comes from using it together with other keyed data. Joining data together is probably one of the most common operations on a pair RDD.\n",
    "\n",
    "- Perform an inner join between two RDDs.\n",
    "\n",
    "- This takes in 2 Pair RDDs, and returns 1 Pair Rdd whose keys are present in both input RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'url_3', (<pyspark.resultiterable.ResultIterable at 0x7f009cbc55d0>, 1.0)),\n",
       " (u'url_2', (<pyspark.resultiterable.ResultIterable at 0x7f009cbb9d90>, 1.0)),\n",
       " (u'url_1', (<pyspark.resultiterable.ResultIterable at 0x7f009cbb9f10>, 1.0)),\n",
       " (u'url_4', (<pyspark.resultiterable.ResultIterable at 0x7f009cbb98d0>, 1.0))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_links_rdd.join(ranks).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_iteration = 2\n",
    "for iteration in range(no_of_iteration):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = url_links_rdd.join(ranks).flatMap(\n",
    "        lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_3 has rank: 0.575.\n",
      "url_2 has rank: 0.394375.\n",
      "url_1 has rank: 1.308125.\n",
      "url_4 has rank: 1.7225.\n"
     ]
    }
   ],
   "source": [
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))\n",
    "    \n",
    "#The higher the rank, more relevant is the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = ranks.collect()\n",
    "import matplotlib.pyplot as plt\n",
    "x, y = zip(*data)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding the operations we performed:\n",
    "\n",
    "<img src = \"images/pagerank_conclusion.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

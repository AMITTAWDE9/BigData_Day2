{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics to be covered:**\n",
    "\n",
    "- Master-worker architecture\n",
    "- Parallel processing: Role of executors\n",
    "- Role of Driver and Cluster manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming with RDDs\tCommon transformation and actions\n",
    "\tLazy evaluation\n",
    "\tRDD execution\n",
    "\tTypes of transformations on RDD\n",
    "\tHands-on problem on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "**Apache Spark is a cluster computing platform designed to be fast and general-purpose.**\n",
    "\n",
    "- On the speed side, Spark extends the popular MapReduce model to efficiently support more types of computations, including interactive queries and stream processing.\n",
    "\n",
    "- One of the main features Spark offers for speed is the ability to run computations in memory, but the system is also more efficient than MapReduce for complex applications running on disk.\n",
    "\n",
    "- At its core, Spark is a **“computational engine”** that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/apache-spark-vs-hadoop-mapreduce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark: Data Science tasks\n",
    "\n",
    "- Data scientists job includes experience with SQL, statistics, predictive modeling (machine learning), and programming, usually in Python, Matlab, or R. Data scientists also have experience with techniques necessary to transform data into formats that can be analyzed for insights (sometimes referred to as data wrangling)\n",
    "\n",
    "\n",
    "- Spark’s speed and simple APIs can make their life bit easy, and its built-in libraries mean that many algorithms are available out of the box.\n",
    "\n",
    "- The Spark shell makes it easy to do interactive data analysis using Python or Scala.\n",
    "\n",
    "- Spark SQL also has a separate SQL shell that can be used to do data exploration using SQL, or Spark SQL can be used as part of a regular Spark program or in the Spark shell.\n",
    "\n",
    "- Machine learning and data analysis is supported through the MLLib libraries or you can load your own models too.\n",
    "\n",
    "- Spark provides a simple way to parallelize these applications across clusters, and hides the complexity of distributed systems programming, network communication, and fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage layers for Spark\n",
    "\n",
    "- Spark can create distributed datasets from any file stored in the Hadoop distributed filesystem (HDFS) or other storage systems supported by the Hadoop APIs (including local filesystem, Amazon S3, Cassandra, Hive, HBase, etc.).\n",
    "\n",
    "*Note: Spark does not require Hadoop; it simply has support for storage sys‐\n",
    "tems implementing the Hadoop APIs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Visit Apache Spark Downloads page\n",
    "\n",
    "    http://spark.apache.org/downloads.html\n",
    "    \n",
    "\n",
    "2. Select following options\n",
    "    1. Choose a Spark release: **2.2.x** or greater (I'll be using 2.2.1)\n",
    "    2. Choose a package type:  **Pre-built for Apache Hadoop 2.7 and later**\n",
    "    3. Download Spark: **spark-2.2.1-bin-hadoop2.7.tgz**\n",
    "\n",
    "    Download that tar compressed file to your local machine.\n",
    "\n",
    "3. After downloading the compressed file, unzip it to desired location:\n",
    "\n",
    "    `$ tar -xvzf spark-2.2.1-bin-hadoop2.7.tgz -C /home/prakshi/spark/`\n",
    "\n",
    "4. Setting up the environment for Spark:\n",
    "\n",
    "    To set up environment variable:\n",
    "    \n",
    "    Add following lines to your `~/.bashrc`\n",
    "\n",
    "    ```bash\n",
    "    export SPARK_HOME=/home/prakshi/tools/spark\n",
    "    export PATH=$SPARK_HOME/bin:$PATH\n",
    "    ```\n",
    "    Make sure you change the path in `SPARK_HOME` as per your spark software file are located.\n",
    "    Reload your `~/.bashrc` file using:\n",
    "\n",
    "    ```\n",
    "    $ source ~/.bashrc\n",
    "    ```\n",
    "\n",
    "5. That's all! Spark has been set-up. Try running `pyspark` command to use Spark from Python.\n",
    "\n",
    "\n",
    "## Pyspark in Jupyter Notebook\n",
    "\n",
    "Two methods to do so.\n",
    "\n",
    "1. Configure PySpark driver\n",
    "    Update PySpark driver environment variables: add these lines to your ~/.bashrc (or ~/.zshrc) file.\n",
    "\n",
    "    ```bash\n",
    "    export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "    export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "    ```\n",
    "    \n",
    "    Restart your terminal and launch PySpark again:\n",
    "\n",
    "    `$ pyspark`\n",
    "\n",
    "    Now, this command should start a Jupyter Notebook in your web browser.\n",
    "\n",
    "2. Using `findspark` module\n",
    "    \n",
    "    findSpark package is not specific to Jupyter Notebook, you can use this trick in your favorite IDE too.\n",
    "\n",
    "    To install findspark:\n",
    "\n",
    "    `$ pip install findspark`\n",
    "\n",
    "    Irrespective of Jupyter notebook/Python script all you need to do to use spark is add following line in your code:\n",
    "\n",
    "    ```python\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Runtime Architecture\n",
    "\n",
    "### Why to learn underlying architecture?\n",
    "\n",
    "\n",
    "Before writing any piece of code for any spark application, it is a must to understand the cluster computing architecture which uses. The architecture understanding will help in visualizing the parallel processing that occurs inside a spark application.\n",
    "\n",
    "- A delightful treat for all the developers is that writing applications for parallel cluster execution use the same API that on a standalone mode. Means, you can use same `pyspark` script on a standalone mode and cluster mode.\n",
    "\n",
    "\n",
    "### Spark Runtime Architecture\n",
    "\n",
    "\n",
    "- Spark can run on a wide variety of cluster managers (Hadoop YARN, Apache Mesos, and Spark’s own built-in Standalone cluster manager) in both on-premise and cloud deployments.\n",
    "\n",
    "- Spark uses a **Master-Slave architecture** in its cluster mode.\n",
    "\n",
    "<img src=\"images/masterSlave.jpeg\">\n",
    "\n",
    "\n",
    "- Single Master and multiple Slaves.\n",
    "\n",
    "- A Spark application is launched on a set of machines using an external service called a **cluster manager**.\n",
    "\n",
    "- A distributed application is placed in execution by a master using a Central coordinator called **Driver**.\n",
    "\n",
    "- Tasks are the smallest unit of work in Spark. One Spark job is divided into multiple tasks.\n",
    "\n",
    "- Executors on worker nodes are responsible for executing these tasks.\n",
    "\n",
    "Let's get into more details one by one:\n",
    "\n",
    "1. The Driver\n",
    "\n",
    "    - Runs the main () function of the application and is the place where the Spark Context is created.\n",
    "\n",
    "    - It has two main duties:\n",
    "\n",
    "         a.) *Converts User Application into tasks*\n",
    "\n",
    "    - Translates the RDD’s into the execution graph and splits the graph into multiple stages\n",
    "\n",
    "\n",
    "   b.) *Scheduling tasks on executors*\n",
    "- Exposes the information about the running spark application through a Web UI at port 4040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking keys of a PairedRDD\n",
    "url_links_rdd.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the formula given above to calculate the PageRanks\n",
    "\n",
    "**Transformations on a Paired RDD**\n",
    "\n",
    "**`mapValues():`** \n",
    "\n",
    "- When we use map() with a Pair RDD, we get access to both Key & value. There are times we might only be interested in accessing the value(& not key). In those case, we can use mapValues() instead of map().\n",
    "\n",
    "- Apply a function to each value of a pair RDD without changing the key.\n",
    "\n",
    "**`reduceByKey()`**:\n",
    "- Combine values with the same key.\n",
    "\n",
    "- It is a wide operation as it shuffles data from multiple partitions and creates another RDD\n",
    "\n",
    "- Before sending data across the partitions, it also merges the data locally using the same associative function for optimized data shuffling\n",
    "\n",
    "**`join()`**:\n",
    "\n",
    "- Some of the most useful operations we get with keyed data comes from using it together with other keyed data. Joining data together is probably one of the most common operations on a pair RDD.\n",
    "\n",
    "- Perform an inner join between two RDDs.\n",
    "\n",
    "- This takes in 2 Pair RDDs, and returns 1 Pair Rdd whose keys are present in both input RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
